{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35500, 364)\n",
      "(15200, 360)\n",
      "Iter  0\n",
      "Accuracy:  0.687266\n",
      "Loss:  3.78982\n",
      "Valid Accuracy:  0.692697\n",
      "Valid Loss:  3.77482\n",
      "__________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7aa62e9dd85c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\schwarz\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\schwarz\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\schwarz\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\schwarz\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\schwarz\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "\n",
    "INPUTDIR='rnn_31_20s_onehot_nor_trainingdata_100subjects.csv'\n",
    "MODELDIR=\"D:/dnn_SAS/model_new_test/first_model.ckpt\"\n",
    "GRAPHDIR=\"D:/dnn_SAS/model_new_test/\"\n",
    "\n",
    "#########################\n",
    "## input data from csv ##\n",
    "#########################\n",
    "## 36562 ##\n",
    "data = genfromtxt(INPUTDIR, delimiter=',')\n",
    "np.random.shuffle(data)\n",
    "# training data \n",
    "data_train=data[0:35500,:]\n",
    "print(data_train.shape)\n",
    "# np.random.shuffle(data_train)\n",
    "data_x = data_train[0:35500,0:360]\n",
    "data_y = data_train[0:35500,360:364]\n",
    "train_x = data_train[0:15000,0:360]\n",
    "train_y = data_train[0:15000,360:364]\n",
    "\n",
    "\n",
    "# validation dataset \n",
    "val_data_x = data[35500:50700,0:360]\n",
    "val_data_y = data[35500:50700,360:364]\n",
    "print(val_data_x.shape)\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################\n",
    "### visualization ###\n",
    "#####################\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "\n",
    "    with tf.name_scope('stddev'):\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "#####################\n",
    "## RNN Architectur ##\n",
    "#####################\n",
    "def RNN(inputs, weights, biases, name=\"RNN\"):\n",
    "    with tf.name_scope(name):\n",
    "        \n",
    "        with tf.name_scope('reshape_x'):\n",
    "            X = tf.reshape(inputs, [-1, n_input])\n",
    "            X_in = tf.matmul(X, weights['in']) + biases['in'] \n",
    "            X_in_ = tf.reshape(X_in, [-1, time_steps, num_units])\n",
    "            \n",
    "            variable_summaries(weights['in'])\n",
    "            variable_summaries(biases['in'])\n",
    "    \n",
    "    # lstm cell\n",
    "        with tf.variable_scope('lstm_cell'):\n",
    "            lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units, forget_bias=1.0)\n",
    "            lstm_cell_drop = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n",
    "#         init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    \n",
    "    # built LSTM-RNN \n",
    "        with tf.variable_scope('lstm_rnn'):   \n",
    "            outputs1, state = tf.nn.dynamic_rnn(lstm_cell_drop, X_in_, dtype=tf.float32, time_major=False)\n",
    "            result=tf.matmul(state[1],weights['out'])+biases['out']\n",
    "            \n",
    "            variable_summaries(weights['in'])\n",
    "            variable_summaries(biases['in'])\n",
    "        tf.summary.histogram('rnn_output',state[1])    \n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "#######################\n",
    "## define parameters ##\n",
    "#######################\n",
    "time_steps=40       # unrolled through 20 data step time(10s)\n",
    "num_units=80       # hidden LSTM units\n",
    "n_input=9           # features\n",
    "learning_rate=0.001 # learning rate for adam\n",
    "n_classes=4         # classified in 4 classes \n",
    "batch_size=500\n",
    "beta=0.005              # 0.005\n",
    "epoch=3000                 # epoch\n",
    "best_acc_valid = 0         # best accuracy of validation dataset\n",
    "acc_pause_num=0            \n",
    "n_improve_threshold = 200  # threshold for early stop\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "## weights for input and output layer ##\n",
    "########################################\n",
    "\n",
    "weights = {\n",
    "    'in': tf.Variable(tf.random_normal([n_input,num_units])),    # (6, 36)    \n",
    "    'out': tf.Variable(tf.random_normal([num_units, n_classes])) # (36, 4)\n",
    "}\n",
    "biases = {\n",
    "    'in': tf.Variable(tf.constant(0.1, shape=[num_units, ])),    # (36, )\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[n_classes, ]))    # (4, )\n",
    "}\n",
    "\n",
    "\n",
    "#######################\n",
    "## Input Placeholder ##\n",
    "#######################\n",
    "\n",
    "x=tf.placeholder(\"float\",[None,time_steps,n_input],name=\"x_input\")\n",
    "y=tf.placeholder(\"float\",[None,n_classes],name=\"y_input\")\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# output = tf.argmax(pred,1,name=\"output\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    with tf.name_scope(\"loss1\"):\n",
    "#     loss1=tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred),reduction_indices=[1]),name=\"loss1\")\n",
    "        loss1=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y), name=\"loss1\")\n",
    "    tf.summary.scalar(\"loss_cross\", loss1)\n",
    "    \n",
    "    with tf.name_scope(\"loss1\"):\n",
    "        #regularizer\n",
    "        tv = tf.trainable_variables()\n",
    "        l2 = beta * sum(\n",
    "            tf.nn.l2_loss(tv)\n",
    "                for tv in tf.trainable_variables()\n",
    "                if not (\"noreg\" in tv.name or \"Bias\" in tv.name)\n",
    "        )\n",
    "        loss2=loss1+l2\n",
    "    tf.summary.scalar(\"loss_l2reg\", loss2)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss2)\n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(pred,1),tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32),name=\"accuracy\")\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    \n",
    "    \n",
    "summ = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "# Save model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# initialization\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(GRAPHDIR + '/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(GRAPHDIR + '/test')\n",
    "    sess.run(init)\n",
    "    train_writer.add_graph(sess.graph)\n",
    "    test_writer.add_graph(sess.graph)\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        i = 0\n",
    "        while i < len(data_y):\n",
    "            # Using different batch for every iteration\n",
    "            start = i\n",
    "            end = i + batch_size\n",
    "        \n",
    "            batch_x = data_x[start:end]\n",
    "            batch_y = data_y[start:end]\n",
    "            \n",
    "            batch_x=np.reshape(batch_x,(batch_size,time_steps,n_input))\n",
    "\n",
    "            sess.run(opt, feed_dict={x: batch_x, y: batch_y,  keep_prob: 1})\n",
    "            i += batch_size\n",
    "\n",
    "        #######################################################\n",
    "        ## print training data, valid data accuracy and loss ##\n",
    "        #######################################################\n",
    "        \n",
    "        if e % 10==0:\n",
    "            train_x_=np.reshape(train_x,(15000,time_steps,n_input))\n",
    "            train_acc, s1, train_loss = sess.run([accuracy, summ, loss2],feed_dict={x: train_x_, y: train_y, keep_prob: 1})\n",
    "            train_writer.add_summary(s1, e)\n",
    "            print(\"Iter \",e)\n",
    "            print(\"Accuracy: \",train_acc)\n",
    "            print(\"Loss: \",train_loss)\n",
    "#             print(\"__________________\")\n",
    "            \n",
    "        if e % 10==0:\n",
    "            val_data_x=np.reshape(val_data_x,(15200,time_steps,n_input))\n",
    "            [valid_acc, s2, valid_loss]=sess.run([accuracy, summ, loss2], feed_dict={x:val_data_x,y:val_data_y,keep_prob: 1})\n",
    "#             los=sess.run(loss,feed_dict={x:np_val_data_x,y:np_val_data_y,keep_prob: 1})\n",
    "#             print(\"For iter \",e)\n",
    "            test_writer.add_summary(s2, e)\n",
    "            print(\"Valid Accuracy: \",valid_acc)\n",
    "            print(\"Valid Loss: \",valid_loss)\n",
    "            print(\"__________________\")\n",
    "            \n",
    "        \n",
    "        \n",
    "        #############################################\n",
    "        ## save the best model that fit valid data ##\n",
    "        #############################################\n",
    "#         if valid_acc > best_acc_valid:\n",
    "#             best_acc_valid = valid_acc\n",
    "#             save_path = saver.save(sess, \"D:/dnn_SAS/model_new_test/first_3_1_model.ckpt\")\n",
    "#             acc_pause_num = 0\n",
    "#         else:\n",
    "#             acc_pause_num += 1\n",
    "            \n",
    "#         # Stop the training operation if accuracy odoes not improve over 100 epochs\n",
    "#         if acc_pause_num >= n_improve_threshold:\n",
    "\n",
    "#             break    \n",
    "            \n",
    "            \n",
    "train_writer.close()\n",
    "test_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1282\n",
      "360\n",
      "<class 'numpy.ndarray'>\n",
      "Test Accuracy:  0.659906\n",
      "__________________\n"
     ]
    }
   ],
   "source": [
    "NUMS='nums.csv'\n",
    "nums = genfromtxt(NUMS, delimiter=',')\n",
    "file1 = 'final_test_data_003'\n",
    "file2 = '.csv'\n",
    "outfile = 'lstm_rnn_20s_public_result/out_20s'\n",
    "outsoft = 'lstm_rnn_20s_public_result/out_soft_20s'\n",
    "infile = 'lstm_rnn_20s_public_result/in_20s'\n",
    "MODELDIR = \"D:/dnn_SAS/model_new_test/first_3_1_model.ckpt\"\n",
    "\n",
    "P_NUM='003'\n",
    "\n",
    "def softmaxx(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "\n",
    "# for i in range(len(nums)):\n",
    "for i in range(1):\n",
    "#     num=nums[i]\n",
    "#     P_=num_str=np.array2string(num, precision=1)\n",
    "#     P_NUM=P_[0:4]\n",
    "#     patient = genfromtxt(file1 + P_NUM + file2, delimiter=',')\n",
    "    patient = genfromtxt(file1 + file2, delimiter=',')\n",
    "    [h,k]=(patient.shape)\n",
    "    pat_x=patient[0:h,0:360]\n",
    "    pat_y=patient[0:h,360:364]\n",
    "    [a,b]=(pat_x.shape)\n",
    "    print(a)\n",
    "    print(b)\n",
    "    # print(type(pat_y))\n",
    "    ##########################################\n",
    "    with tf.Session() as sess:\n",
    "    \n",
    "        load_path = saver.restore(sess, MODELDIR)\n",
    "        pat_x_=np.reshape(pat_x,(a,time_steps,n_input))\n",
    "        print(type(pat_x_))\n",
    "        new_test_acc=sess.run(accuracy,feed_dict={x:pat_x_,y:pat_y})\n",
    "    \n",
    "        prediction =sess.run(pred, feed_dict={x:pat_x_})\n",
    "        soft_out=softmaxx(prediction)\n",
    "        out=np.argmax(prediction,1)\n",
    "        np.savetxt(outfile + P_NUM + file2, out, delimiter = ',')\n",
    "        np.savetxt(outsoft + P_NUM + file2, soft_out, delimiter = ',')\n",
    "        input_matrix=np.argmax(pat_y,1)\n",
    "        np.savetxt(infile + P_NUM + file2, input_matrix, delimiter = ',')\n",
    "    \n",
    "#     los=sess.run(loss,feed_dict={x:test_data_x,y:test_data_y})\n",
    "        print(\"Test Accuracy: \",new_test_acc)\n",
    "        print(\"__________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-517d8606219c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msoftmaxx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-d4a586599223>\u001b[0m in \u001b[0;36msoftmaxx\u001b[0;34m(z)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msoftmaxx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# necessary step to do broadcasting\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "a=[1,2,3]\n",
    "softmaxx(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
